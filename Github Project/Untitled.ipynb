{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d34a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for data manipulation, analysis, and modeling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns  \n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea57a0b3",
   "metadata": {},
   "source": [
    "## Define the SP500DataHandler Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b724b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class to handle S&P 500 market data\n",
    "class SP500DataHandler:\n",
    "    # Define class-level attributes for column names\n",
    "    DATE_COLUMN = 'Date'\n",
    "    OPEN_COLUMN = 'Open'\n",
    "    HIGH_COLUMN = 'High'\n",
    "    LOW_COLUMN = 'Low'\n",
    "    VOLUME_COLUMN = 'Volume'\n",
    "    ADJ_CLOSE_COLUMN = 'Adj Close'\n",
    "    VOLUME_10DAY_AVG = 'Volume_10day_avg'\n",
    "    ADJ_CLOSE_10DAY_AVG = 'Adj_Close_10day_avg'\n",
    "    PREDICTIONS_COLUMN = 'predictions'\n",
    "    CLOSE_LAST_COLUMN = 'Close/Last'\n",
    "    YEAR_COLUMN = 'Year'\n",
    "    MONTH_COLUMN = 'Month'\n",
    "    DAY_COLUMN = 'Day'\n",
    "    DAY_OF_WEEK_COLUMN = 'DayOfWeek'\n",
    "    DAILY_RETURNS_COLUMN = 'Daily Returns'\n",
    "    VOLATILITY_COLUMN = 'Volatility'\n",
    "    RESIDUALS_COLUMN = 'residuals'\n",
    "    MA50_ADJ_COLUMN = 'MA50_Adj'\n",
    "    MA200_ADJ_COLUMN = 'MA200_Adj'\n",
    "    VWAP_COLUMN = 'VWAP'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5575261f",
   "metadata": {},
   "source": [
    "## Initialize and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56884265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, file_path):\n",
    "        self.file_path = file_path  # File path for the data source\n",
    "        self.data = None  # Placeholder for the data\n",
    "        self.models = {\n",
    "            'LinearRegression': LinearRegression(),\n",
    "            'Ridge': Ridge(),\n",
    "            'Lasso': Lasso(),\n",
    "            'RandomForest': RandomForestRegressor(),\n",
    "            'GradientBoosting': GradientBoostingRegressor()}\n",
    "        self.param_grid = {\n",
    "            'Ridge': {'alpha': [10, 50, 100, 200]},\n",
    "            'Lasso': {'alpha': [0.01, 0.1, 1, 10, 100], 'max_iter': [5000]},\n",
    "            'RandomForest': {'n_estimators': [100, 200, 300], 'max_depth': [10, 20, 30, None]},\n",
    "            'GradientBoosting': {'n_estimators': [100, 200], 'learning_rate': [0.01, 0.1]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c0a1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(self, start_date=None, end_date=None):\n",
    "        try:\n",
    "            # Use DATE_COLUMN class attribute\n",
    "            self.data = pd.read_csv(self.file_path, parse_dates=[self.DATE_COLUMN])\n",
    "            print(\"Data loaded successfully.\")\n",
    "            \n",
    "            # Filter data by date range if both start and end dates are specified\n",
    "            if start_date and end_date:\n",
    "                self.data = self.data[(self.data[self.DATE_COLUMN] >= pd.to_datetime(start_date)) &\n",
    "                                      (self.data[self.DATE_COLUMN] <= pd.to_datetime(end_date))]\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {self.file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c43727",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e426c408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(self):\n",
    "        if self.data is not None:\n",
    "            self.data.dropna(inplace=True)\n",
    "            \n",
    "            # Extract year, month, and day from DATE_COLUMN\n",
    "            self.data[self.YEAR_COLUMN] = self.data[self.DATE_COLUMN].dt.year\n",
    "            self.data[self.MONTH_COLUMN] = self.data[self.DATE_COLUMN].dt.month\n",
    "            self.data[self.DAY_COLUMN] = self.data[self.DATE_COLUMN].dt.day\n",
    "            self.data[self.DAY_OF_WEEK_COLUMN] = self.data[self.DATE_COLUMN].dt.dayofweek\n",
    "            \n",
    "            print(\"Data cleaning and feature engineering completed.\")\n",
    "        else:\n",
    "            print(\"Data not loaded. Please load the data first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c49721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ten_day_avg(self):\n",
    "        if self.VOLUME_COLUMN in self.data.columns and self.ADJ_CLOSE_COLUMN in self.data.columns:\n",
    "            self.data[self.VOLUME_10DAY_AVG] = self.data[self.VOLUME_COLUMN].rolling(window=10).mean()\n",
    "            self.data[self.ADJ_CLOSE_10DAY_AVG] = self.data[self.ADJ_CLOSE_COLUMN].rolling(window=10).mean()\n",
    "            \n",
    "            # Replace infinite values with NaN and then fill them with column mean\n",
    "            self.data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "            self.data.fillna(self.data.mean(), inplace=True)\n",
    "            \n",
    "            print(\"Feature engineering completed: 10-day averages added.\")\n",
    "        else:\n",
    "            print(\"Required columns for feature engineering are missing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95e9ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_daily_returns_and_volatility(self):\n",
    "            # Calculate daily returns and volatility for risk assessment\n",
    "            self.data[self.DAILY_RETURNS_COLUMN] = self.data[self.CLOSE_LAST_COLUMN].pct_change()\n",
    "            self.data[self.VOLATILITY_COLUMN] = self.data[self.DAILY_RETURNS_COLUMN].rolling(window=30).std() * np.sqrt(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fd3aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(self):\n",
    "        # Perform additional feature engineering to enhance the dataset\n",
    "        if self.data is not None:\n",
    "            # Calculate moving averages for 'Adj Close' to smooth out price data\n",
    "            self.data[self.MA50_ADJ_COLUMN] = self.data[self.ADJ_CLOSE_COLUMN].rolling(window=50).mean()\n",
    "            self.data[self.MA200_ADJ_COLUMN] = self.data[self.ADJ_CLOSE_COLUMN].rolling(window=200).mean()\n",
    "\n",
    "            # Calculate Volume-Weighted Average Price (VWAP) as an additional feature\n",
    "            cum_vol_price = (self.data[self.VOLUME_COLUMN] * self.data[self.ADJ_CLOSE_COLUMN]).cumsum()\n",
    "            cum_volume = self.data[self.VOLUME_COLUMN].cumsum()\n",
    "            self.data[self.VWAP_COLUMN] = cum_vol_price / cum_volume\n",
    "\n",
    "            print(\"Feature engineering completed successfully.\")\n",
    "        else:\n",
    "            print(\"Data not loaded. Please load and clean the data before feature engineering.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8d2b9e",
   "metadata": {},
   "source": [
    "## Model Selection and Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108b3a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(self):\n",
    "        X = self.data[[self.OPEN_COLUMN, self.HIGH_COLUMN, self.LOW_COLUMN, self.VOLUME_10DAY_AVG, self.ADJ_CLOSE_10DAY_AVG]]\n",
    "        y = self.data[self.ADJ_CLOSE_COLUMN]\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4fe00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_and_tune_model(self, cv=5):\n",
    "        scaler = StandardScaler()\n",
    "        best_score = float('inf')\n",
    "        best_model_name = None\n",
    "        best_model = None\n",
    "\n",
    "        for name, model in self.models.items():\n",
    "            print(f\"Processing {name}...\")\n",
    "            pipeline = Pipeline([('scaler', scaler), ('model', model)])\n",
    "            if name in self.param_grid:\n",
    "                # Set up GridSearchCV\n",
    "                adjusted_param_grid = {'model__' + key: value for key, value in self.param_grid[name].items()}\n",
    "                grid_search = GridSearchCV(model, self.param_grid[name], cv=cv, scoring='neg_mean_squared_error')\n",
    "                grid_search.fit(self.X_train, self.y_train)\n",
    "                print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n",
    "                print(f\"Best cross-validation score (MSE) for {name}: {-grid_search.best_score_}\")\n",
    "\n",
    "                if -grid_search.best_score_ < best_score:\n",
    "                    best_score = -grid_search.best_score_\n",
    "                    best_model_name = name\n",
    "                    best_model = grid_search.best_estimator_\n",
    "            else:\n",
    "                # For models without a parameter grid, fit the default model and perform cross-validation\n",
    "                scores = cross_val_score(model, self.X_train, self.y_train, cv=cv, scoring='neg_mean_squared_error')\n",
    "                avg_score = -scores.mean()\n",
    "                print(f\"Average cross-validation score (MSE) for {name}: {avg_score}\")\n",
    "\n",
    "                if avg_score < best_score:\n",
    "                    best_score = avg_score\n",
    "                    best_model_name = name\n",
    "                    model.fit(self.X_train, self.y_train)\n",
    "                    best_model = model\n",
    "\n",
    "        self.best_model_name = best_model_name\n",
    "        self.best_model = best_model\n",
    "        self.best_score = best_score\n",
    "\n",
    "        print(f\"Best model: {self.best_model_name} with MSE: {self.best_score}\")\n",
    "        self.predictions = self.best_model.predict(self.X_test)\n",
    "\n",
    "        # Initialize a column for predictions in the dataset and fill it for the test set\n",
    "        self.data[self.PREDICTIONS_COLUMN] = np.nan\n",
    "        self.data.loc[self.X_test.index, self.PREDICTIONS_COLUMN] = self.predictions\n",
    "\n",
    "        # Evaluate the best model using common regression metrics\n",
    "        self.mse = mean_squared_error(self.y_test, self.predictions)\n",
    "        self.mae = mean_absolute_error(self.y_test, self.predictions)\n",
    "        self.r2 = r2_score(self.y_test, self.predictions)\n",
    "\n",
    "        print(f\"Evaluation Metrics for {self.best_model_name}: MSE={self.mse}, MAE={self.mae}, R2={self.r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc433f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_cross_validation(self, cv=5):\n",
    "        # Perform cross-validation to assess the model's performance\n",
    "        try:\n",
    "            # Ensure the model has been fitted\n",
    "            if not hasattr(self.best_model, 'fit'):\n",
    "                raise ValueError(\"Model must be fitted before performing cross-validation.\")\n",
    "\n",
    "            cv_scores = cross_val_score(self.best_model, self.X_train, self.y_train, cv=cv, scoring='neg_mean_squared_error')\n",
    "            self.cv_mse_scores = -cv_scores  # Convert scores to positive values\n",
    "            self.avg_cv_mse = np.mean(self.cv_mse_scores)  # Calculate average MSE\n",
    "            print(f\"Cross-validation performed with {cv} folds. Average MSE: {self.avg_cv_mse:.2f}\")\n",
    "        except ValueError as ve:\n",
    "            print(f\"Value Error: {ve}\")\n",
    "        except NotFittedError:\n",
    "            print(\"Model is not fitted. Fit the model before performing cross-validation.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error during cross-validation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02584549",
   "metadata": {},
   "source": [
    "## Machine Learning Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b57228a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(self):\n",
    "        # Visualize the actual vs. predicted values\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(self.y_test, self.predictions, alpha=0.5)\n",
    "        plt.plot([self.y_test.min(), self.y_test.max()], [self.y_test.min(), self.y_test.max()], 'k--', lw=2)\n",
    "        plt.xlabel('Actual')\n",
    "        plt.ylabel('Predicted')\n",
    "        plt.title('Actual vs. Predicted Values')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041fc068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(self):\n",
    "        # Visualize the importance of features used in the linear regression model\n",
    "        importance = self.best_model.coef_\n",
    "        features = [self.OPEN_COLUMN, self.HIGH_COLUMN, self.LOW_COLUMN, self.VOLUME_10DAY_AVG, self.ADJ_CLOSE_10DAY_AVG]\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(features, importance)\n",
    "        plt.xlabel('Feature')\n",
    "        plt.ylabel('Coefficient Value')\n",
    "        plt.title('Feature Importance')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547c82a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_store_residuals(self):\n",
    "        # Calculate and store residuals between actual closing prices and predictions\n",
    "        if self.PREDICTIONS_COLUMN in self.data.columns and self.CLOSE_LAST_COLUMN in self.data.columns:\n",
    "            self.data[self.RESIDUALS_COLUMN] = self.data[self.CLOSE_LAST_COLUMN] - self.data[self.PREDICTIONS_COLUMN]\n",
    "            print(\"Residuals calculated and stored.\")\n",
    "        else:\n",
    "            missing_columns = [col for col in [self.PREDICTIONS_COLUMN, self.CLOSE_LAST_COLUMN] if col not in self.data.columns]\n",
    "            print(f\"Required columns {missing_columns} are missing in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1d1459",
   "metadata": {},
   "source": [
    "## Returning Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835f56ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_data(self):\n",
    "        # Return the cleaned and preprocessed data\n",
    "        if self.data is not None:\n",
    "            return self.data\n",
    "        else:\n",
    "            print(\"Data is not ready. Please ensure data is loaded, cleaned, and preprocessed.\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe6255d",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91ad75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of the SP500DataHandler class\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = 'SPX_2.csv'  # Update this to the actual path of your dataset\n",
    "    data_handler = SP500DataHandler(file_path)\n",
    "    data_handler.load_data(start_date='1960-08-01', end_date='1985-12-31')\n",
    "    data_handler.clean_data()\n",
    "    data_handler.ten_day_avg()\n",
    "    data_handler.calculate_daily_returns_and_volatility()\n",
    "    data_handler.feature_engineering()\n",
    "    data_handler.split_data()\n",
    "    data_handler.select_and_tune_model()\n",
    "    data_handler.perform_cross_validation()\n",
    "    data_handler.visualize_predictions()\n",
    "    data_handler.feature_importance()\n",
    "    data_handler.calculate_and_store_residuals()\n",
    "    clean_data = data_handler.get_clean_data()\n",
    "\n",
    "    \n",
    "    # Display the head of the cleaned data if available\n",
    "    if clean_data is not None:\n",
    "        print(clean_data.head())\n",
    "    else:\n",
    "        print(\"No data to display.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
